{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Bug Hunting with Pytest**\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DOCTESTS:**\n",
    "\n",
    "##### Doctests é uma das formas de escrever e executar testes unitários em Python, uma biblioteca nativa, criando _docstrings_ (strings de documentação).\n",
    "\n",
    "**Além dos mesmo benefícios dos testes comuns, ele possuí outras vantagens como:**\n",
    "\n",
    "* _Uma maneira mais rápida e fácil, de criar testes e documentar uma função, uma vez que estão integrados._\n",
    "\n",
    "* _Facilidade para seguir o TDD, desenvolvimento orientado a testes._\n",
    "\n",
    "**_Um ponto importante de resaltar é que o doctest é mais orientado a testar funções locais, mais simples com entradas e saídas definidas._**\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Escrevendo Doctests:\n",
    "\n",
    "**Os doctests utilizam a sintaxe exemplos do REPL do Python: `>>> `, e para executar: `python3 -m doctest nome_do_arquivo.py`**\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean(numbers):\n",
    "    \"\"\"\n",
    "    Calcula a média de uma lista de números.\n",
    "\n",
    "    >>> my_list = [1, 2, 3, 4, 5]\n",
    "    >>> mean(my_list)\n",
    "    3.0\n",
    "    >>> mean([2.5, 3.75, 1.25, 4])\n",
    "    2.875\n",
    "    >>> mean([])\n",
    "    0\n",
    "\n",
    "    \"\"\"\n",
    "    return sum(numbers) / len(numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### O retorno deste teste resultará em um erro:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "source": [
    "~~~bash\n",
    "**********************************************************************\n",
    "File \"(_caminho do arquivo_)\", line 10, in arquivo-testes.mean\n",
    "Failed example:\n",
    "    mean([])\n",
    "Exception raised:\n",
    "    Traceback (most recent call last):\n",
    "      File \"/usr/lib/python3.10/doctest.py\", line 1350, in __run\n",
    "        exec(compile(example.source, filename, \"single\",\n",
    "      File \"<doctest arquivo-testes.mean[3]>\", line 1, in <module>\n",
    "        mean([])\n",
    "      File \"(_caminho do arquivo_)\", line 14, in mean\n",
    "        return sum(numbers) / len(numbers)\n",
    "    ZeroDivisionError: division by zero\n",
    "**********************************************************************\n",
    "1 items had failures:\n",
    "   1 of   4 in arquivo-testes.mean\n",
    "***Test Failed*** 1 failures.\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Há duas possibilidades para a correção deste erro:**\n",
    "\n",
    "* _A primeira seria utilizando o try/catch com o except ZeroDivionError, retornando 0:_\n",
    "\n",
    "~~~python\n",
    "try:\n",
    "        return sum(numbers) / len(numbers)\n",
    "    except ZeroDivisionError:\n",
    "        return 0\n",
    "~~~\n",
    "\n",
    "* _Outra forma seria utilizando a constante Ellipsis do Python `...`, que indicam que outras possíveis saídas não são importantes:_\n",
    "\n",
    "~~~python\n",
    ">>> mean([])\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ZeroDivisionError: division by zero\n",
    "\n",
    "    \"\"\"\n",
    "~~~\n",
    "\n",
    "<br>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pytest:**\n",
    "\n",
    "##### O Pytest é uma das estruturas mais utilizadas para testes unitários em Python, sua sintaxe simples e fácil juntamente com sua capacidade de executar testes em paralelo, detectar testes automáticamente, pular testes, são algumas de suas vantagens.\n",
    "\n",
    "**Instalando o Pytest:**\n",
    "\n",
    "* _Crie o ambiente virtual com o comando: `python3 -m venv .venv.`._\n",
    "\n",
    "* _Ative o ambiente virtual com o comando: `source .venv/bin/activate`._\n",
    "\n",
    "* _Instale o Pytest com o comando: `pip install pytest==7.3.1`._\n",
    "\n",
    "* _Confirme que a instalação foi bem sucedida rodando o comando: `pytest --version`._\n",
    "\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Criando testes:**\n",
    "\n",
    "##### Existem 2 formas de criar um arquivo de testes, ambas as formas são aceitas, sendo um critério do usuário:\n",
    "\n",
    "* _Criando um arquivo Python que comece com `test_`, sendo mais fácio de identificar os arquivos._\n",
    "\n",
    "* _Outra forma é criando o arquivo Python mas com o final sendo `_test.py`, sendo mais fácil a busca por ordem alfabética e facilitando a visualização._\n",
    "\n",
    "\n",
    "**Os arquivos de testes podem estar separados em um sub-diretório, uma vez que o o Pytest faz uma busca em todos os diretórios dentro do atual. É comum os testes estarem em uma pasta chamada `tests`.**\n",
    "\n",
    "##### Os testes em si, a função, deve começar com o prefixo `test_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arquivo_test.py\n",
    "\n",
    "def test_a_simple_test():\n",
    "    assert True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Utilizando o comando `pytest`,  ou o comando `pytest -vv`, executará o arquivo de teste acima, que terá este retorno:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "~~~bash\n",
    "=========================================================== test session starts ===========================================================\n",
    "platform linux -- Python 3.10.12, pytest-7.3.1, pluggy-1.5.0\n",
    "rootdir: \"(_caminho do arquivo_)\"\n",
    "collected 1 item                                                                                                                          \n",
    "\n",
    "arquivo_test.py .                                                                                                                   [100%]\n",
    "\n",
    "============================================================ 1 passed in 0.01s ============================================================\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Se alterar no código o `assert` para ser `False`, o retorno será:** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~bash\n",
    "=========================================================== test session starts ===========================================================\n",
    "platform linux -- Python 3.10.12, pytest-7.3.1, pluggy-1.5.0\n",
    "rootdir: \"(_caminho do arquivo_)\"\n",
    "collected 1 item                                                                                                                          \n",
    "\n",
    "arquivo_test.py F                                                                                                                   [100%]\n",
    "\n",
    "================================================================ FAILURES =================================================================\n",
    "___________________________________________________________ test_a_simple_test ____________________________________________________________\n",
    "\n",
    "    def test_a_simple_test():\n",
    ">       assert False\n",
    "E       assert False\n",
    "\n",
    "arquivo_test.py:2: AssertionError\n",
    "========================================================= short test summary info =========================================================\n",
    "FAILED arquivo_test.py::test_a_simple_test - assert False\n",
    "============================================================ 1 failed in 0.02s ============================================================\n",
    "~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rodando Doctests com Pytest:**\n",
    "\n",
    "**O Pytest é capaz de rodar além dos arquivos de testes, os Doctests feitos nos arquivos de testes, só é necessário incluir a flag `doctest-modules`, no comando de execução dos testes. No retorno o doctests é tratado como um teste só.**\n",
    "\n",
    "**`pytest --doctest-modules -vv`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arquivo_test.py\n",
    "\n",
    "def mean(numbers):\n",
    "    \"\"\"\n",
    "    Calcula a média de uma lista de números.\n",
    "\n",
    "    >>> my_list = [1, 2, 3, 4, 5]\n",
    "    >>> mean(my_list)\n",
    "    3.0\n",
    "    >>> mean([2.5, 3.75, 1.25, 4])\n",
    "    2.875\n",
    "    >>> mean([])\n",
    "    Traceback (most recent call last):\n",
    "    ...\n",
    "    ZeroDivisionError: division by zero\n",
    "\n",
    "    \"\"\"\n",
    "    return sum(numbers) / len(numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "~~~bash\n",
    "=================== test session starts ====================\n",
    "collected 3 items                                          \n",
    "\n",
    "a_test.py::test_a_simple_test PASSED                 [ 33%]\n",
    "a_test.py::test_list_multiply PASSED                 [ 66%]\n",
    "main.py::main.mean PASSED                            [100%]\n",
    "\n",
    "==================== 3 passed in 0.01s =====================\n",
    "~~~\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Rodando os testes no Vs Code:**\n",
    "\n",
    "##### O Vs Code tem um menu exclusivo para testes, que tem abrange uma variedade de linguagens. Neste menu você pode observar cada teste que foi executado, executar testes específicos, ou um conjunto de testes, também é possível rodar o \"debugger\" direto no teste.\n",
    "\n",
    "**Para integrar o pytest com o Vs Code é necessário:**\n",
    "\n",
    "* _Precisa ter a extensão do Python instalada._\n",
    "\n",
    "* _Adicionar no arquivo do Vs Code \"Preferences: Open User Settings (JSON)\" o seguinte código dentro da chave de configuração:_\n",
    "\n",
    "~~~json\n",
    "{\n",
    "    \"python.testing.pytestEnabled\": true, // Habilita o pytest\n",
    "    \"python.testing.pytestArgs\": [ // Argumentos do pytest\n",
    "        \"--doctest-modules\", // Procura por doctests em arquivos .py\n",
    "        \"-vv\", // Aumenta o nível de verbosidade\n",
    "    ],\n",
    "}\n",
    "~~~\n",
    "\n",
    "**Você pode abrir a janela de testes por meio do menu `View -> Testing`.**\n",
    "\n",
    "* _Para executar um teste específico, clique no botão triangular ao lado do teste. Para executar todos os testes em um arquivo ou em todo o projeto, clique no botão triangular acima do nome do arquivo ou do projeto, respectivamente._\n",
    "\n",
    "* _Há um botão em forma de triângulo com um inseto (bug), que permite abrir o \"debugger\" para um teste ou conjunto de testes. Há também um ícone de arquivo que abre o arquivo de teste na linha do teste._\n",
    "\n",
    "* _Ao clicar com o botão direito no triângulo ao lado de cada teste no arquivo, você pode ver opções para executar o teste, abrir o debugger no teste ou adicionar breakpoints._\n",
    "\n",
    "<br>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fixtures:**\n",
    "\n",
    "##### As fixtures são funções que podem rodar antes e/ou depois dos testes. Podem definir e criar dados a serem usados nos testes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arquivo_test.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture # Criamos a fixture por meio do decorador pytest.fixture\n",
    "def my_list(): # Por padrão, o nome da fixture será o nome da função\n",
    "    return [1, 2, 3] # Retorna o valor que a fixture possuirá\n",
    "\n",
    "\n",
    "def test_a_simple_test():\n",
    "    assert True\n",
    "\n",
    "\n",
    "def test_sum(my_list): # Recebemos a fixture como parâmetro da função de teste\n",
    "    assert sum(my_list) == 6 # Usamos a lista retornada pela fixture\n",
    "\n",
    "\n",
    "def test_list_item_multiply(my_list): # Recebemos a mesma fixture aqui também\n",
    "    assert [item * 3 for item in my_list] == [3, 6, 9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Acima a fixture é declarada e utilizada em outras funções.\n",
    "\n",
    "**Observação:**\n",
    "\n",
    "#### Os Decoradores são uma poderosa ferramenta em Python, pois permitem estender e modificar o comportamento de funções e classes de forma flexível e modular, melhorando a legibilidade e a reutilização do código. No caso acima, a criação da função pytest.fixture, o que é evidenciado pela sintaxe que utiliza o símbolo `@`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arquivo_test.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture # Criamos a fixture por meio do decorador pytest.fixture\n",
    "def my_list(): # Por padrão, o nome da fixture será o nome da função\n",
    "    return [1, 2, 3] # Retorna o valor que a fixture possuirá\n",
    "\n",
    "\n",
    "def test_a_simple_test():\n",
    "    assert True\n",
    "\n",
    "\n",
    "def test_sum(my_list): # Recebemos a fixture como parâmetro da função de teste\n",
    "    assert sum(my_list) == 6 # Usamos a lista retornada pela fixture\n",
    "\n",
    "\n",
    "def test_list_item_multiply(my_list): # Recebemos a mesma fixture aqui também\n",
    "    assert [item * 3 for item in my_list] == [3, 6, 9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fixture em outros arquivos:**\n",
    "\n",
    "##### Para usar uma fixture em vários arquivos de teste, você pode definir a fixture em um arquivo chamado `conftest.py`. O Pytest automaticamente reconhece as fixtures definidas neste arquivo, e as torna disponíveis para todos os arquivos de teste no mesmo diretório e nos subdiretórios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conftest.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture\n",
    "def my_list():\n",
    "    return [1, 2, 3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# arquivo_test.py\n",
    "\n",
    "def test_a_simple_test():\n",
    "    assert True\n",
    "\n",
    "\n",
    "def test_sum(my_list):\n",
    "    assert sum(my_list) == 6\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outro_test.py\n",
    "\n",
    "def test_list_item_multiply(my_list):\n",
    "    assert [item * 3 for item in my_list] == [3, 6, 9]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Escopos da fixtures:**\n",
    "\n",
    "##### O escopo das fixtures do pytest determina em qual momento uma fixture será inicializada e finalizada durante a execução dos testes, e é definido por um parâmetro na sua declaração ou no `conftest.py`.\n",
    "\n",
    "**Para mudar o escopo, basta passar o escopo desejado como argumento para o parâmetro scope do decorador `pytest.fixture`, como `@pytest.fixture(scope=\"module\")`.**\n",
    "\n",
    "* _function: é inicializada a cada função de teste que a utiliza._\n",
    "\n",
    "* _module: é inicializada uma vez para cada módulo de teste que a utiliza._\n",
    "\n",
    "* _class: é inicializada uma vez para cada classe de teste que a utiliza._\n",
    "\n",
    "* _package: é inicializada apenas uma vez para cada diretório que contém arquivos de teste._\n",
    "\n",
    "* _session: é inicializada apenas uma vez, no início da execução da suíte de testes._\n",
    "\n",
    "**Se uma fixture cria DB para um teste, a criação desse objeto pode ser custosa, então é melhor não recriá-lo para cada teste. No entanto, se um teste modifica o objeto do banco de dados, isso pode afetar outros testes que dependem da mesma fixture. Portanto, uma nova instância da fixture é criada para cada teste para evitar efeitos colaterais indesejados, já que idealmente um teste nunca deve depender de outro teste para passar.**\n",
    "\n",
    "<br>\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Fixtures built-in:**\n",
    "\n",
    "##### O Pytest conta com alguns várias fixtures nativas pronto para uso.\n",
    "\n",
    "**3 tipos de fixtures nativas:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Capsys:** _<br> Usada para capturar as saídas padrão e de erro em um teste, sendo possível verificar se a saída de está correta ou fazer asserções nas messagens de erro. Esta função escreve no stdout, e posseu um método `readouterr`, que lê as saidas e erros, retorando um objeto contendo `err` e `out`. Ela também pode ser usada para capturar a saída de erro padrão `stderr`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sim, é só receber `capsys` como parâmetro em qualquer função de teste que o\n",
    "# pytest faz o resto da magia acontecer\n",
    "def test_print_to_stdout(capsys):\n",
    "    print(\"Hello, world!\")\n",
    "    captured = capsys.readouterr()\n",
    "    assert captured.out == \"Hello, world!\\n\"  # print coloca \\n automaticamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_error_to_stderr(capsys):\n",
    "    import sys\n",
    "    sys.stderr.write(\"Error message\\n\")\n",
    "    captured = capsys.readouterr()\n",
    "    assert captured.err == \"Error message\\n\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Monkeypatch:** _<br> Usada para alterar o comportamento de funções ou métodos, permitindo a simulação de condições específicas para testes, o acesso ao objeto \"patch\". Ao testar uma função que chama a função `input()`, esta fixture simula a entrada da pessoa usuária sem precisar realmente digitar. Para fazer isso, você pode usar o método `setattr` e substituir o objeto input da biblioteca padrão por uma função que retorna uma string de entrada simulada._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_function():\n",
    "    return f\"Você digitou {input('Digite algo: ')}!\"\n",
    "\n",
    "\n",
    "def test_my_function(monkeypatch):\n",
    "    # Input recebe um parâmetro que mock_input não usa, por isso o _\n",
    "    def mock_input(_):\n",
    "        return \"Python\"\n",
    "\n",
    "    # Trocamos a input do sistema pela nossa mock_input\n",
    "    monkeypatch.setattr(\"builtins.input\", mock_input)\n",
    "    output = my_function()\n",
    "\n",
    "    assert output == \"Você digitou Python!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Tmp_path:** _<br> Usada para criar um diretório temporário em que um teste pode criar e manipular arquivos, encarregando se de criar e limpar o diretório temporário automaticamente antes e depois dos testes a que utilizam. Esta fixture retorna um objeto `pathlib.Path`, que pode ser utilizado como uma string de caminho para um diretório. Seu uso pode ser para testar uma função que cria arquivos, por exemplo uma função que gera um arquivo de saída. O teste pode criar um diretório temporário com a fixture e chamar a função a ser testada, passando o diretório temporário como argumento._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def generate_output(content, path):\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(json.dumps(content))\n",
    "\n",
    "\n",
    "def test_generate_output(tmp_path):\n",
    "    content = {\"a\": 1}\n",
    "    output_path = tmp_path / \"out.json\"\n",
    "    # O operador '/' funciona na linha anterior pois temp_path não é uma\n",
    "    # string comum, mas sim um objeto Path\n",
    "\n",
    "    generate_output(content, output_path)\n",
    "\n",
    "    assert os.path.isfile(output_path)\n",
    "    with open(output_path) as file:\n",
    "        assert file.read() == '{\"a\": 1}'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Makers:**\n",
    "\n",
    "##### Os markers (marcadores) no Pytest são uma forma de marcar testes com atributos específicos que podem ser usados para executar, filtrar ou pular testes, e podem ser definidos usando a sintaxe `@pytest.mark.nome_do_marker` no código de teste.\n",
    "\n",
    "##### Para criar um marcador, basta criar uma função que tenha como argumento um objeto de tipo `pytest.mark`, como um marcador chamado “slow” dentro de um arquivo de testes, definindo a seguinte função. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow_marker():\n",
    "    time.sleep(4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Executando o seguinte comando `pytest -m MARKEXPR`, no qual MARKEXPR é uma expressão que adiciona ou remove a seleção de um ou mais marcadores, ao rodar `pytest -m 'not slow' -vv` são executados todos os testes, menos os marcados como slow.\n",
    "\n",
    "**Resultado do código acima:**\n",
    "\n",
    "~~~bash\n",
    "================================================= test session starts =================================================\n",
    "platform darwin -- Python 3.11.0, pytest-7.3.1, pluggy-1.0.0\n",
    "collected 8 items / 1 deselected / 7 selected                                                                         \n",
    "\n",
    "a_test.py::test_a_simple_test PASSED                                                                            [ 14%]\n",
    "a_test.py::test_sum PASSED                                                                                      [ 28%]\n",
    "another_test.py::test_list_item_multiply PASSED                                                                 [ 42%]\n",
    "fixtures_test.py::test_print_to_stdout PASSED                                                                   [ 57%]\n",
    "fixtures_test.py::test_error_to_stderr PASSED                                                                   [ 71%]\n",
    "fixtures_test.py::test_my_function PASSED                                                                       [ 85%]\n",
    "fixtures_test.py::test_generate_output PASSED                                                                   [100%]\n",
    "\n",
    "================================================== warnings summary ===================================================\n",
    "markers_test.py:6\n",
    "  ./pytest_app/markers_test.py:6: PytestUnknownMarkWarning: Unknown pytest.mark.slow - is this a typo?\n",
    "    You can register custom marks to avoid this warning\n",
    "    for details, see https://docs.pytest.org/en/stable/how-to/mark.html\n",
    "    @pytest.mark.slow\n",
    "\n",
    "-- Docs: https://docs.pytest.org/en/stable/how-to/capture-warnings.html\n",
    "===================================== 7 passed, 1 deselected, 1 warning in 0.02s ======================================\n",
    "~~~\n",
    "\n",
    "**É importante observar que com o marcador criado é necessário informar ao Pytest sobre sua existência, sendo possível adicionar o código no `conftest.py`,**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conftest.py\n",
    "\n",
    "def pytest_configure(config):\n",
    "    config.addinivalue_line(\n",
    "        \"markers\", \"slow: marks tests as slow\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Makers built-in:**\n",
    "\n",
    "##### Assim como as fixtures, o Pytest já traz alguns marcadores embutidos.\n",
    "\n",
    "* **Skip:** _<br> O marcador `skip` serve para pular um teste específico. Sua variante mais específica `skipif`, que pula um teste a depender de uma condição._\n",
    "\n",
    "\n",
    "* **XFail:** _<br> O marcador `XFail` serve para que um teste falhe propositalmente, expect fail._\n",
    "\n",
    "<br>\n",
    "\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercícios:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1º Exercício:\n",
    "\n",
    "\n",
    "##### _A função sum_two_numbers abaixo contém um bug. Crie um exemplo na docstring que pega esse bug ao rodar o módulo doctest e, em seguida, corrija-o._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_two_numbers(a, b):\n",
    "    \"\"\"Retorna a soma de dois números recebidos por parâmetro.\n",
    "\n",
    "    Exemplos\n",
    "    --------\n",
    "    >>> sum_two_numbers(0, 0)\n",
    "    0\n",
    "    \"\"\"\n",
    "    return a - b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### R: A única alteração a ser feita é a troca do sinal no return de `-` para `+`, uma vez que se espera a soma, para observar o retorno basta trocar os valores da função `sum_two_numbers()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
